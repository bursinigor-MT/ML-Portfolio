# README.md

## О проекте

В этом репозитории показан процесс решения задачи прогноза вовлеченности пользователей.
Необходимо было определить, досмотрит ли пользователь видео более чем на 50%, то есть стояла задача бинарной классификации.
В ходе анализа данных и построения признаков были выполнены целевые преобразования, обработка пропусков, отбор и генерация признаков, а также обучение ансамбля моделей XGBoost и LightGBM с последующим блендингом их предсказаний.
Финальный результат составил F1-score = **97.65%**.

## Предобработка и EDA
- Удалены столбцы `for_kids` и `studios` — в обоих >96% пропусков, практически нет сигнала.  
- Удалены столбцы `title`, `title_orig` — отсутствие адекватного отклика модели.  
- Из датасета удалено **144 строки**, где целевая переменная **NaN**.  
- Пропуски в `sex`, `age`, `income` заменены на класс-заглушку `UNKNOWN`.  
- Объединены `income`, `income_90_150`, `income_150_inf` в один признак.  

---

## Feature engineering
- Преобразование категорий возраста и дохода в средние значения (например, `age_18_24` → `21`, `income_0_20` → `10` и т.д.)  
- `last_watch_dt` → создан признак `last_d_count_watch`: число дней с последнего просмотра относительно максимальной даты.  
- `release_year` → создан признак `film_age` и его категориальная версия (`new`, `mid_age_film`, `old`).  
- `total_dur` переведён в часы, логарифмирован и разбит на 5 категорий: `very_short`, `short`, `mid`, `long`, `very_long`.  
- `actors`, `keywords`, `directors`, `countries`, `genres` — FrequencyEncoding с L1-нормой (из вектора категорий получали скаляр).  

---

## Модели
Использовались два бустинга с косинусным расписанием обучения:  
- **XGBoost**  
- **LightGBM**  

**Параметры XGBoost:**
```
n_estimators=1000
eta=0.1
reg_lambda=1
reg_alpha=0.003
max_depth=18
colsample_bytree=0.8
subsample=0.9
```
**Параметры LightGBM:**
```
n_estimators=2000
learning_rate=0.05
max_depth=18
num_leaves=30
reg_alpha=1
reg_lambda=0.003
n_jobs=-1
```

---

## Результаты
- XGBoost выделил самой важной фичей `total_dur` и производные.  
- LightGBM — `content_type`, `total_dur`, `film_age`.  
- Обе модели по отдельности: **F1-score ≈ 97.4+%**  
- Финальный блендинг: **F1-score = 97.65%**  

---
## Интерпретация результатов
При выборе признаков основная цель состояла в том, чтобы максимально отразить поведение пользователей и характеристики контента, влияющие на вероятность просмотра видео более чем на 50%. Одним из ключевых факторов стал признак total_dur, поскольку именно он в наибольший отклик у моделей. Интересно, что в этом датасете наблюдалась обратная от интуитивной зависимость — пользователи чаще досматривали длинные видео. Изначально мы предполагали, что чаще будут досматриваться более короткие видео, также были подозрения насчет особенно длинных роликов, которые, как оказалось, досматривались наполовину.  
Использование блединга моделей XGBoost и LightGBM позволило выявить сложные нелинейные зависимости, объеденить сильные стороны обеих моделей и сократить количество ошибок. XGBoost оказался чувствительным к категориальным переменным, таким как длительность фильма, LightGBM откликался на частотно закодированные переменные и числовые признаки. 

---
## Графики важности признаков
![XGBoost Feature Importance](xgb_fi.jpg)
![LightGBM Feature Importance](lgbm_fi.jpg)

---
